![Maintenance](https://img.shields.io/maintenance/yes/2021.svg?color=red&style=plastic)
![GitHub last commit](https://img.shields.io/github/last-commit/tao-bai/attack-and-defense-methods.svg?style=plastic)
[![Awesome](https://awesome.re/badge.svg?style=flat-square)](https://awesome.re)
<!-- [![GitHub commit activity](https://img.shields.io/github/commit-activity/m/tao-bai/attack-and-defense-methods.svg?style=flat-square?foo=bar)](https://img.shields.io/github/commit-activity/y/tao-bai/attack-and-defense-methods?style=plastic) -->

# About
Inspired by [this repo](https://github.com/aleju/papers) and [ML Writing Month](https://docs.google.com/document/d/15o6m0I8g6O607mk5YPTh33Lu_aQYo7SpHhNSbLPQpWQ/mobilebasic?from=groupmessage#?utm_source=wechat_session&utm_medium=social&utm_oi=624560843380101120). Questions and discussions are most welcome!

[Lil-log](https://lilianweng.github.io/lil-log/) is the best blog I have ever read!
# Papers

## Survey
1. `TNNLS 2019` [Adversarial Examples: Attacks and Defenses for Deep Learning](https://ieeexplore.ieee.org/document/8611298)
2. `IEEE ACCESS 2018` [Threat of Adversarial Attacks on Deep Learning in Computer Vision: A Survey](https://ieeexplore.ieee.org/document/8294186)
3. `2019` [Adversarial Attacks and Defenses in Images, Graphs and Text: A Review](https://arxiv.org/pdf/1909.08072)
4. `2019` [A Study of Black Box Adversarial Attacks in Computer Vision](https://arxiv.org/pdf/1912.01667)
5. `2019` [Adversarial Examples in Modern Machine Learning: A Review](https://arxiv.org/pdf/1911.05268)
6. `2020` [Opportunities and Challenges in Deep Learning Adversarial Robustness: A Survey](https://arxiv.org/abs/2007.00753)
7. `TPAMI 2021` [Knowledge Distillation and Student-Teacher Learning for Visual Intelligence: A Review and New Outlooks](https://arxiv.org/pdf/2004.05937)
8. `2019` [Adversarial attack and defense in reinforcement learning-from AI security view](https://arxiv.org/pdf/1901.06796)
9. `2020` [A Survey of Privacy Attacks in Machine Learning](https://arxiv.org/abs/2007.07646)
10. `2020` [Learning from Noisy Labels with Deep Neural Networks: A Survey](https://arxiv.org/abs/2007.08199)
11. `2020` [Optimization for Deep Learning: An Overview](https://link.springer.com/epdf/10.1007/s40305-020-00309-6?sharing_token=Xv0f6yBzgc1QnNAUbQ9pufe4RwlQNchNByi7wbcMAY56wZ54Vxigc8CL-kHvhiYpSthXAu14ZSiMmkrVuqUSJUCRoWymQqZbEnVDQvz2sEBOiX8dkkGxS7bI7irClme0cEKnUtpyPIJONJQQDAiWTskwNws64eAd2xKnqi3nYOY%3D)
12. `2020` [Backdoor Attacks and Countermeasures on Deep Learning: A Comprehensive Review](https://arxiv.org/abs/2007.10760)
13. `2020` [Learning from Noisy Labels with Deep Neural Networks: A Survey](https://arxiv.org/pdf/2007.08199)
14. `2020` [Adversarial Machine Learning in Image Classification: A Survey Towards the Defender's Perspective](https://arxiv.org/pdf/2009.03728)
15. `2020` [Efficient Transformers: A Survey](https://arxiv.org/abs/2009.06732)
16. `2019` [A Survey of Black-Box Adversarial Attacks on Computer Vision Models](https://arxiv.org/abs/1912.01667)
17. `2020` [Backdoor Learning: A Survey](https://arxiv.org/abs/2007.08745)
18. `2020` [Transformers in Vision: A Survey](https://arxiv.org/abs/2101.01169)
19. `2020` [A Survey on Neural Network Interpretability](https://arxiv.org/abs/2012.14261)
20. `2020`[A Survey of Privacy Attacks in Machine Learning](https://arxiv.org/abs/2007.07646)
21. `2020` [Dataset Security for Machine Learning: Data Poisoning, Backdoor Attacks, and Defenses](https://arxiv.org/pdf/2012.10544)
22. `2021` [**Recent Advances in Adversarial Training for Adversarial Robustness**](https://arxiv.org/abs/2102.01356) (**Our work, accepted by IJCAI 2021**)
23. `2021` [Explainable Artificial Intelligence Approaches: A Survey](https://arxiv.org/abs/2101.09429)
24. `2021` [A Survey on Understanding, Visualizations, and Explanation of Deep Neural Networks](https://arxiv.org/abs/2102.01792)
25. `2020` [A survey on Semi-, Self- and Unsupervised Learning for Image Classification](https://arxiv.org/abs/2002.08721)
26. `2021` [Model Complexity of Deep Learning: A Survey](https://arxiv.org/abs/2103.05127)
27. `2021` [Deep Generative Modelling: A Comparative Review of VAEs, GANs, Normalizing Flows, Energy-Based and Autoregressive Models](https://arxiv.org/abs/2103.04922)
28. `2021` [Dataset Security for Machine Learning: Data Poisoning, Backdoor Attacks, and Defenses](https://arxiv.org/abs/2012.10544)
29. `2019` [Advances and Open Problems in Federated Learning](https://arxiv.org/pdf/1912.04977.pdf)
30. `2021` [Countering Malicious DeepFakes: Survey, Battleground, and Horizon](https://arxiv.org/pdf/2103.00218.pdf)

## Attack
### 2013
1. `ICLR` [Evasion Attacks against Machine Learning at Test Time](./2013/Evasion_attacks_against_machine_learning_at_test_time.md)


### 2014
1. `ICLR` [Intriguing properties of neural networks](./2014/Intriguing_properties_of_neural_networks.md)
2. `ARXIV` [Identifying and attacking the saddle point problem in
  high-dimensional non-convex optimization]


### 2015
1. `ICLR` [Explaining and Harnessing Adversarial Examples](./2015/Explaining_and_Harnessing_Adversarial_Examples.md)


### 2016
1. `EuroS&P` [The limitations of deep learning in adversarial settings](./2016/The_limitations_of_deep_learning_in_adversarial_settings.md)
2. `CVPR` [Deepfool](./2016/DeepFool.md)
3. `SP` [C&W Towards evaluating the robustness of neural networks](./2016/Toward_evaluating_the_robustness_of_neural_networks.md)
4. `Arxiv` [Transferability in machine learning: from phenomena to black-box attacks using adversarial samples](./2016/Transferability_in_machine_learning.md)
5. `NIPS` [Adversarial Images for Variational Autoencoders]
6. `ARXIV` [A boundary tilting persepective on the phenomenon of adversarial examples]
7. `ARXIV` [Adversarial examples in the physical world]


### 2017
1. `ICLR` [Delving into Transferable Adversarial Examples and Black-box Attacks](./2017/Delving_into_Transferable_Adversarial_Examples_and_Black-box_Attacks.md)
2. `CVPR` [Universal Adversarial Perturbations](./2017/Universal_Adversarial_Perturbations.md)
3. `ICCV` [Adversarial Examples for Semantic Segmentation and Object Detection](./2017/Adversarial_Examples_for_Semantic_Segmentation_and_Object_Detection.md)
4. `ARXIV` [Adversarial Examples that Fool Detectors](./2017/Adversarial_Examples_that_Fool_Detectors.md)
5. `CVPR` [A-Fast-RCNN: Hard Positive Generation via Adversary for Object Detection](./2017/A-Fast-RCNN_Hard_Positive_Generation_via_Adversary_for_Object_Detection.md)
6. `ICCV` [Adversarial Examples Detection in Deep Networks with Convolutional Filter Statistics](./2017/Adversarial_Examples_Detection_in_Deep_Networks_with_Convolutional_Filter_Statistics.md)
7. `AIS` [Adversarial examples are not easily detected: Bypassing ten detection methods]
8. `ICCV` `UNIVERSAL` [Universal Adversarial Perturbations Against Semantic Image Segmentation]
9. `ICLR` [Adversarial Machine Learning at Scale]
10. `ARXIV` [The space of transferable adversarial examples]
11. `ARXIV` [Adversarial attacks on neural network policies]


### 2018
1. `ICLR` [Generating Natural Adversarial Examples](./2018/Generating_Natural_Adversarial_Examples.md)
2. `NeurlPS` [Constructing Unrestricted Adversarial Examples with Generative Models](./2018/Constructing_Unrestricted_Adversarial_Examples_with_Generative_Models.md)
3.  `IJCAI` [Generating Adversarial Examples with Adversarial Networks](./2018/Generating_Adversarial_Examples_with_Adversarial_Networks.md)
4.  `CVPR` [Generative Adversarial Perturbations](./2018/Generative_Adversarial_Perturbations.md)
5.  `AAAI` [Learning to Attack: Adversarial transformation networks](./2017/Adversarial_transformation_networks_Learning_to_generate_adversarial_examples.md)
6.  `S&P` [Learning Universal Adversarial Perturbations with Generative Models](./2018/Learning_Universal_Adversarial_Perturbations_with_Generative_Models.md)
7.  `CVPR` [Robust physical-world attacks on deep learning visual classification](./2018/Robust_physical_world_attacks_on_deep_learning_visual_classification.md)
8.  `ICLR` [Spatially Transformed Adversarial Examples](./2018/SPATIALLY_TRANSFORMED_ADVERSARIAL_EXAMPLES.md)
9.  `CVPR`[Boosting Adversarial Attacks With Momentum](./2018/Boosting_Adversarial_Attacks_With_Momentum.md)
10. `ICML` [Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples](./2018/Obfuscated_Gradients_Give_a_False_Sense_of_Security_Circumventing_Defenses_to_Adversarial_Examples.md) :thumbsup:
11. `CVPR` `UNIVERSAL` [Art of Singular Vectors and Universal Adversarial Perturbations]
12. `ARXIV` [Adversarial Spheres]
13. `ECCV` [Characterizing adversarial examples based on spatial consistency information for semantic segmentation]
14. `ARXIV` [Generating natural language adversarial examples]
15. `SP` [Audio adversarial examples: Targeted attacks on speech-to-text]
16. `ARXIV` [Adversarial attack on graph structured data]
17. `ARXIV` [Maximal Jacobian-based Saliency Map Attack (Variants of JAMA)] 
18. `SP` [Exploiting Unintended Feature Leakage in Collaborative Learning]


### 2019
1. `CVPR` [Feature Space Perturbations Yield More Transferable Adversarial Examples](./2019/Feature_Space_Perturbations_Yield_More_Transferable_Adversarial_Examples.md)
2. `ICLR` [The Limitations of Adversarial Training and the Blind-Spot Attack](./2019/The_Limitations_of_Adversarial_Training_and_the_Blind-Spot_Attack.md)
3.  `ICLR` [Are adversarial examples inevitable?](./2019/Are_adversarial_examples_inevitable.md) :thought_balloon:
4.  `IEEE TEC` [One pixel attack for fooling deep neural networks](./2019/One_pixel_attack_for_fooling_deep_neural_networks.md)
5.  `ARXIV` [Generalizable Adversarial Attacks Using Generative Models](./2019/Generalizable_Adversarial_Attacks_Using_Generative_Models.md)
6.  `ICML` [NATTACK: Learning the Distributions of Adversarial Examples for an Improved Black-Box Attack on Deep Neural Networks](./2019/NATTACK_Learning_the_Distributions_of_Adversarial_Examples_for_an_Improved_Black_Box_Attack_on_Deep_Neural_Networks.md):thought_balloon:
7.  `ARXIV` [SemanticAdv: Generating Adversarial Examples via Attribute-conditional Image Editing](./2019/SemanticAdv_Generating_Adversarial_Examples_via_Attribute_conditional_Image_Editing.md)
8.  `CVPR` [Rob-GAN: Generator, Discriminator, and Adversarial Attacker](./2019/Rob_GAN_Generator_Discriminator_and_Adversarial_Attacker.md)
9.  `ARXIV` [Cycle-Consistent Adversarial {GAN:} the integration of adversarial attack and defense](./2019/Cycle_Consistent_Adversarial_{GAN}_the_integration_of_adversarial_attack_and_defense.md)
10. `ARXIV` [Generating Realistic Unrestricted Adversarial Inputs using Dual-Objective {GAN} Training](./2019/Generating_Realistic_Unrestricted_Adversarial_Inputs_using_Dual_Objective_{GAN}_Training.md) :thought_balloon:
11. `ICCV` [Sparse and Imperceivable Adversarial Attacks](./2019/Sparse_and_Imperceivable_Adversarial_Attacks.md):thought_balloon:
12. `ARXIV` [Perturbations are not Enough: Generating Adversarial Examples with Spatial Distortions](2019/Perturbations_are_not_Enough_Generating_Adversarial_Examples_with_Spatial_Distortions.md)
13. `ARXIV` [Joint Adversarial Training: Incorporating both Spatial and Pixel Attacks](2019/Joint_Adversarial_Training_Incorporating_both_Spatial_and_Pixel_Attacks.md)
14. `IJCAI` [Transferable Adversarial Attacks for Image and Video Object Detection](./2019/Transferable_Adversarial_Attacks_for_Image_and_Video_Object_Detection.md)
15. `TPAMI` [Generalizable Data-Free Objective for Crafting Universal Adversarial Perturbations](./2019/Generalizable_Adversarial_Attacks_Using_Generative_Models.md)
16. `CVPR` [Decoupling Direction and Norm for Efficient Gradient-Based L2 Adversarial Attacks and Defenses](./2019/Decoupling_Direction_and_Norm_for_Efficient_Gradient_Based_L2_Adversarial_Attacks_and_Defenses.md)
17. `CVPR` [FDA: Feature Disruptive Attack]
18. `ARXIV` [SmoothFool: An Efficient Framework for Computing Smooth Adversarial Perturbations]
19. `CVPR` [SparseFool: a few pixels make a big difference]
20. `ICLR` [Adversarial Attacks on Graph Neural Networks via Meta Learning]
21. `NeurIPS` [Deep Leakage from Gradients]
22. `CCS` [Comprehensive Privacy Analysis of Deep Learning: Passive and Active White-box Inference Attacks against Centralized and Federated Learning]
23. `ICCV` [Universal Perturbation Attack Against Image Retrieval]
24. `ICCV` [Enhancing Adversarial Example Transferability with an Intermediate Level Attack]
25. `CVPR` [Evading Defenses to Transferable Adversarial Examples by Translation-Invariant Attacks]
26. `ICLR` [ADef: an Iterative Algorithm to Construct Adversarial Deformations]
27. `Neurips` [iDLG: Improved deep leakage from gradients.]
28. `ARXIV` [Reversible Adversarial Attack based on Reversible Image Transformation]
29. `CCS` [Seeing isn’t Believing: Towards More Robust Adversarial Attack Against Real World Object Detectors]

### 2020
1. `ICLR` [Fooling Detection Alone is Not Enough: Adversarial Attack against Multiple Object Tracking](./2020/Fooling_Detection_Alone_is_Not_Enough_Adversarial_Attack_against_Multiple_Object_Tracking.md):thought_balloon:
2. `ARXIV` [Sponge Examples: Energy-Latency Attacks on Neural Networks]
3. `ICML` [Minimally distorted Adversarial Examples with a Fast Adaptive Boundary Attack]
4. `ICML` [Stronger and Faster Wasserstein Adversarial Attacks]
5. `CVPR` [QEBA: Query-Efﬁcient Boundary-Based Blackbox Attack]
6. `ECCV` [New Threats Against Object Detector with Non-local Block]
7. `ARXIV` [Towards Imperceptible Universal Attacks on Texture Recognition]
8. `ECCV` [Frequency-Tuned Universal Adversarial Attacks]
9. `AAAI` [Learning Transferable Adversarial Examples via Ghost Networks]
10. `ECCV` [SPARK: Spatial-aware Online Incremental Attack Against Visual Tracking]
11. `Neurips` [Inverting Gradients - How easy is it to break privacy in federated learning?]
12. `ICLR` [Nesterov Accelerated Gradient and Scale Invariance for Adversarial Attacks]
13. `NeurIPS` [On Adaptive Attacks to Adversarial Example Defenses]
14. `AAAI` [Beyond Digital Domain: Fooling Deep Learning Based Recognition System in Physical World]
15. `ARXIV` [Adversarial Color Enhancement: Generating Unrestricted Adversarial Images by Optimizing a Color Filter]
16. `CVPR` [Adversarial Camouflage: Hiding Physical-World Attacks With Natural Styles]
17. `CVPR` [Universal Physical Camouflage Attacks on Object Detectors] [code](https://github.com/mesunhlf/UPC-tf)
18. `ARXIV` [Understanding Object Detection Through An Adversarial Lens]

### 2021
1. `ARXIV` [On Generating Transferable Targeted Perturbations]
2. `CVPR` [See through Gradients: Image Batch Recovery via GradInversion] :thumbsup:
3. `ARXIV` [Admix: Enhancing the Transferability of Adversarial Attacks]
4. `ARXIV` [Deep Image Destruction: A Comprehensive Study on Vulnerability of Deep Image-to-Image Models against Adversarial Attacks]
5. `ARXIV` [Poisoning the Unlabeled Dataset of Semi-Supervised Learning] **Carlini**
6. `ARXIV` [AdvHaze: Adversarial Haze Attack]
7. `CVPR` [LAFEAT : Piercing Through Adversarial Defenses with Latent Features](https://zhuanlan.zhihu.com/p/370521833)
8. `ARXIV` [IMPERCEPTIBLE ADVERSARIAL EXAMPLES FOR FAKE IMAGE DETECTION]
9. `ICME` [TRANSFERABLE ADVERSARIAL EXAMPLES FOR ANCHOR FREE OBJECT DETECTION]
10. `ICLR` [Unlearnable Examples: Making Personal Data Unexploitable]
11. `ICMLW` [Detecting Adversarial Examples Is (Nearly) As Hard As Classifying Them]
12. `ARXIV` [Mischief: A Simple Black-Box Attack Against Transformer Architectures]
13. `ECCV` [Patch-wise Attack for Fooling Deep Neural Network]
14. `ICCV` [Naturalistic Physical Adversarial Patch for Object Detectors]


## Defence
### 2014
1. `ARXIV`  [Towards deep neural network architectures robust to adversarial examples](2014/Towards_deep_neural_network_architectures_robust_to_adversarial_examples.md)

### 2015
1. [Learning with a strong adversary]
2. [IMPROVING BACK-PROPAGATION BY ADDING AN ADVERSARIAL GRADIENT]
3. [Distributional Smoothing with Virtual Adversarial Training]


### 2016
1. `NIPS` [Robustness of classifiers: from adversarial to random noise](./2016/Robustness_of_classifiers_from_adversarial_to_random_noise.md) :thought_balloon:

### 2017
1. `ARXIV` [Countering Adversarial Images using Input Transformations](./2017/Countering_Adversarial_Images_using_Input_Transformations.md)
2. `ICCV` [SafetyNet: Detecting and Rejecting Adversarial Examples Robustly]
3. `Arxiv` [Detecting adversarial samples from artifacts](./2017/Detecting_Adversarial_Samples_from_Artifacts.md)
4. `ICLR` [On Detecting Adversarial Perturbations](./2017/On_Detecting_Adversarial_Perturbations.md) :thought_balloon:
5. `ASIA CCS` [Practical black-box attacks against machine learning]
6. `ARXIV` [The space of transferable adversarial examples]
7. `ICCV` [Adversarial Examples for Semantic Segmentation and Object Detection]

### 2018
1. `ICLR` [Defense-{GAN}: Protecting Classifiers Against Adversarial Attacks Using Generative Models](./2018/Defense-{GAN}_Protecting_Classifiers_Against_Adversarial_Attacks_Using_Generative_Models.md)
2. . `ICLR` [Ensemble Adversarial Training: Attacks and Defences](./2018/Ensemble_Adversarial_Training_Attacks_and_Defenses.md)
3.  `CVPR` [Defense Against Universal Adversarial Perturbations](./2018/Defense_Against_Universal_Adversarial_Perturbations.md)
4.  `CVPR` [Deflecting Adversarial Attacks With Pixel Deflection](./2018/Deflecting_Adversarial_Attacks_With_Pixel_Deflection.md)
5.  `TPAMI` [Virtual adversarial training: a regularization method for supervised and semi-supervised learning](./2018/Virtual_adversarial_training_a_regularization_method_for_supervised_and_semi_supervised_learning.md) :thought_balloon:
6.  `ARXIV` [Adversarial Logit Pairing](./2018/Adversarial_Logit_Pairing.md)
7.  `CVPR` [Defense Against Adversarial Attacks Using High-Level Representation Guided Denoiser](./2018/Defense_Against_Adversarial_Attacks_Using_High_Level_Representation_Guided_Denoiser.md)
8.  `ARXIV` [Evaluating and understanding the robustness of adversarial logit pairing](./2018/Evaluating_and_understanding_the_robustness_of_adversarial_logit_pairing.md)
9.  `CCS` [Machine Learning with Membership Privacy Using Adversarial Regularization](./2018/Machine_Learning_with_Membership_Privacy_Using_Adversarial_Regularization.md)
10. `ARXIV` [On the robustness of the cvpr 2018 white-box adversarial example defenses]
11. `ICLR` [Thermometer Encoding: One Hot Way To Resist Adversarial Examples]
12. `IJCAI` [Curriculum Adversarial Training]
13. `ICLR` [Countering Adversarial Images using Input Transformations]
14. `CVPR` [Defense Against Adversarial Attacks Using High-Level Representation Guided Denoiser]
15. `ICLR` [Towards Deep Learning Models Resistant to Adversarial Attacks]
16. `AAAI` [Improving the Adversarial Robustness and Interpretability of Deep Neural Networks by Regularizing Their Input Gradients]
17. `NIPS` [Adversarially robust generalization requires more data]
18. `ARXIV` [Is robustness the cost of accuracy? - {A} comprehensive study on the robustness of 18 deep image classification models.]
19. `ARXIV` [Robustness may be at odds with accuracy]
20. `ICLR` [PIXELDEFEND: LEVERAGING GENERATIVE MODELS TO UNDERSTAND AND DEFEND AGAINST ADVERSARIAL EXAMPLES]

### 2019
1. `NIPS` [Adversarial Training and Robustness for Multiple Perturbations](./2019/Adversarial_Training_and_Robustness_for_Multiple_Perturbations.md)
2. `NIPS` [Adversarial Robustness through Local Linearization](./2019/Adversarial_Robustness_through_Local_Linearization.md)
3.  `CVPR` [Retrieval-Augmented Convolutional Neural Networks against Adversarial Examples](./2019/Retrieval_Augmented_Convolutional_Neural_Networks_against_Adversarial_Examples.md)
4.  `CVPR` [Feature Denoising for Improving Adversarial Robustness](./2019/Feature_Denoising_for_Improving_Adversarial_Robustness.md)
5.  `NEURIPS` [A New Defense Against Adversarial Images: Turning a Weakness into a Strength](./2019/A_New_Defense_Against_Adversarial_Images_Turning_a_Weakness_into_a_Strength.md)
6.  `ICML` [Interpreting Adversarially Trained Convolutional Neural Networks](./2019/Interpreting_Adversarially_Trained_Convolutional_Neural_Networks.md)
7.  `ICLR` [Robustness May Be at Odds with Accuracy](./2019/Robustness_May_Be_at_Odds_with_Accuracy.md):thought_balloon:
8.  `IJCAI` [Improving the Robustness of Deep Neural Networks via Adversarial Training with Triplet Loss](./2019/Improving_the_Robustness_of_Deep_Neural_Networks_via_Adversarial_Training_with_Triplet_Loss.md)
9.  `ICML` [Adversarial Examples Are a Natural Consequence of Test Error in Noise](./2019/Adversarial_Examples_Are_a_Natural_Consequence_of_Test_Error_in_Noise.md):thought_balloon:
10. `ICML` [On the Connection Between Adversarial Robustness and Saliency Map Interpretability](./2019/On_the_Connection_Between_Adversarial_Robustness_and_Saliency_Map_Interpretability.md)
11. `NeurIPS` [Metric Learning for Adversarial Robustness](./2019/Metric_Learning_for_Adversarial_Robustness.md)
12. `ARXIV` [Defending Adversarial Attacks by Correcting logits](./2019/Defending_Adversarial_Attacks_by_Correcting_logits.md)
13. `ICCV` [Adversarial Learning With Margin-Based Triplet Embedding Regularization](./2019/Adversarial_Learning_With_Margin_Based_Triplet_Embedding_Regularization.md)
14. `ICCV` [CIIDefence: Defeating Adversarial Attacks by Fusing Class-Specific Image Inpainting and Image Denoising](./2019/CIIDefence_Defeating_Adversarial_Attacks_by_Fusing_Class_Specific_Image_Inpainting_and_Image_Denoising.md)
15. `NIPS` [Adversarial Examples Are Not Bugs, They Are Features](./2019/Adversarial_Examples_Are_Not_Bugs_They_Are_Features.md)
16. `ICML` [Using Pre-Training Can Improve Model Robustness and Uncertainty](./2019/Using_Pre_Training_Can_Improve_Model_Robustness_and_Uncertainty.md)
17. `NIPS` [Defense Against Adversarial Attacks Using Feature Scattering-based Adversarial Training](./2019/Defense_Against_Adversarial_Attacks_Using_Feature_Scattering_based_Adversarial_Training.md):thought_balloon:
18. `ICCV` [Improving Adversarial Robustness via Guided Complement Entropy](/2019/Improving_Adversarial_Robustness_via_Guided_Complement_Entropy.md)
19. `NIPS` [Robust Attribution Regularization](./2019/Robust_Attribution_Regularization.md) :thought_balloon:
20. `NIPS` [Are Labels Required for Improving Adversarial Robustness?](./2019/Are_Labels_Required_for_Improving_Adversarial_Robustness.md)
21. `ICLR` [Theoretically Principled Trade-off between Robustness and Accuracy](./2019/Theoretically_Principled_Trade_off_between_Robustness_and_Accuracy.md)
22. `CVPR` [Adversarial defense by stratified convolutional sparse coding]
23. `ICML` [On the Convergence and Robustness of Adversarial Training]
24. `CVPR` [Robustness via Curvature Regularization, and Vice Versa]
25. `CVPR` [ComDefend: An Efficient Image Compression Model to Defend Adversarial Examples]
26. `ICML` [Improving Adversarial Robustness via Promoting Ensemble Diversity]
27. `ICML` [Towards the first adversarially robust neural network model on {MNIST}]
28. `NIPS` [Unlabeled Data Improves Adversarial Robustness]
29. `ICCV` [Evaluating Robustness of Deep Image Super-Resolution Against Adversarial Attacks]
30. `ICML` [Using Pre-Training Can Improve Model Robustness and Uncertainty]
31. `ARXIV` [Improving adversarial robustness of ensembles with diversity training]
32. `ICML` [Adversarial Robustness Against the Union of Multiple Perturbation Models]
33. `CVPR` [Robustness via Curvature Regularization, and Vice Versa]
34. `NIPS` [Robustness to Adversarial Perturbations in Learning from Incomplete Data]
35. `ICML` [Improving Adversarial Robustness via Promoting Ensemble Diversity]
36. `NIPS` [Adversarial Robustness through Local Linearization]
37. `ARXIV` [Adversarial training can hurt generalization]
38. `NIPS` [Adversarial training for free!]
39. `ICLR` [Improving the generalization of adversarial training with domain adaptation]
40. `CVPR` [Disentangling Adversarial Robustness and Generalization]
41. `NIPS` [Adversarial Training and Robustness for Multiple Perturbations]
42. `ICCV` [Bilateral Adversarial Training: Towards Fast Training of More Robust Models Against Adversarial Attacks]
43. `ICML` [On the Convergence and Robustness of Adversarial Training]
44. `ICML` [Rademacher Complexity for Adversarially Robust Generalization]
45. `ARXIV` [Adversarially Robust Generalization Just Requires More Unlabeled Data]
46. `ARXIV` [You only propagate once: Accelerating adversarial training via maximal principle]
47. `NIPS` [Cross-Domain Transferability of Adversarial Perturbations](./2019/Cross_Domain_Transferability_of_Adversarial_Perturbations.md)
48. `ARXIV` [Adversarial Robustness as a Prior for Learned Representations]
49. `ICLR` [Structured Adversarial Attack: Towards General Implementation and Better Interpretability]
50. `ICLR` [Defensive Quantization: When Efficiency Meets Robustness]
51. `NeurIPS` [A New Defense Against Adversarial Images: Turning a Weakness into a Strength]

### 2020
1. `ICLR` [Jacobian Adversarially Regularized Networks for Robustness](./2020/Jacobian_Adversarially_Regularized_Networks_for_Robustness.md)
2. `CVPR` [What it Thinks is Important is Important: Robustness Transfers through Input Gradients](./2020/What_it_Thinks_is_Important_is_Important_Robustness_Transfers_through_Input_Gradients.md)
3.  `ICLR` [Adversarially Robust Representations with Smooth Encoders](2020/Adversarially_Robust_Representations_with_Smooth_Encoders.md) :thought_balloon:
4.  `ARXIV` [Heat and Blur: An Effective and Fast Defense Against Adversarial Examples](./2020/Heat_and_Blur_An_Effective_and_Fast_Defense_Against_Adversarial_Examples.md)
5.  `ICML` [Triple Wins: Boosting Accuracy, Robustness and Efficiency Together by Enabling Input-Adaptive Inference](./2020/Triple_Wins_Boosting_Accuracy_Robustness_and_Efficiency_Together_by_Enabling_Input_Adaptive_Inference.md)
6.  `CVPR` [Wavelet Integrated CNNs for Noise-Robust Image Classification](./2020/Wavelet_Integrated_CNNs_for_Noise_Robust_Image_Classification.md)
7.  `ARXIV` [Deflecting Adversarial Attacks](./2020/Deflecting_Adversarial_Attacks.md)
8.  `ICLR` [Robust Local Features for Improving the Generalization of Adversarial Training](./2020/Robust_Local_Features_for_Improving_the_Generalization_of_Adversarial_Training.md)
9.  `ICLR` [Enhancing Transformation-Based Defenses Against Adversarial Attacks with a Distribution Classifier](./2020/Enhancing_Transformation_Based_Defenses_Against_Adversarial_Attacks_with_a_Distribution_Classifier.md)
10. `CVPR` [A Self-supervised Approach for Adversarial Robustness](./2020/A_Self_supervised_Approach_for_Adversarial_Robustness.md)
11. `ICLR` [Improving Adversarial Robustness Requires Revisiting Misclassified Examples](./2019/Improving_the_Robustness_of_Deep_Neural_Networks_via_Adversarial_Training_with_Triplet_Loss.md) :thumbsup:
12. `ARXIV` [Manifold regularization for adversarial robustness](2020/Manifold_regularization_for_adversarial_robustness.md)
13. `NeurIPS` [DVERGE: Diversifying Vulnerabilities for Enhanced Robust Generation of Ensembles](./2020/DVERGE_Diversifying_Vulnerabilities_for_Enhanced_Robust_Generation_of_Ensembles.md)
14. `ARXIV` [A Closer Look at Accuracy vs. Robustness](./2020/A_Closer_Look_at_Accuracy_vs_Robustness.md)
15. `NeurIPS` [Energy-based Out-of-distribution Detection](./2020/Energy_based_Out_of_distribution_Detection.md)
16. `ARXIV` [Out-of-Distribution Generalization via Risk Extrapolation (REx)](./2020/Out_of_Distribution_Generalization_via_Risk_Extrapolation.md)
17. `CVPR` [Adversarial Examples Improve Image Recognition](./2020/Adversarial_Examples_Improve_Image_Recognition.md)
18. `ICML` [Confidence-Calibrated Adversarial Training: Generalizing to Unseen Attacks] :thumbsup:
19. `ICML` [Efficiently Learning Adversarially Robust Halfspaces with Noise]
20. `ICML` [Implicit Euler Skip Connections: Enhancing Adversarial Robustness via Numerical Stability]
21. `ICML` [Friendly Adversarial Training: Attacks Which Do Not Kill Training Make Adversarial Learning Stronger]
22. `ICML` [Learning Adversarially Robust Representations via Worst-Case Mutual Information Maximization] :thumbsup:
23. `ICML` [Overfitting in adversarially robust deep learning] :thumbsup:
24. `ICML` [Proper Network Interpretability Helps Adversarial Robustness in Classification]
25. `ICML` [Randomization matters How to defend against strong adversarial attacks]
26. `ICML` [Reliable Evaluation of Adversarial Robustness with an Ensemble of Diverse Parameter-free Attacks]
27. `ICML` [Towards Understanding the Regularization of Adversarial Robustness on Neural Networks]
28. `CVPR` [Defending Against Universal Attacks Through Selective Feature Regeneration]
29. `ARXIV` [Understanding and improving fast adversarial training]
30. `ARXIV` [Cat: Customized adversarial training for improved robustness]
31. `ICLR` [MMA Training: Direct Input Space Margin Maximization through Adversarial Training]
32. `ARXIV` [Bridging the performance gap between fgsm and pgd adversarial training]
33. `CVPR` [Adversarial Vertex Mixup: Toward Better Adversarially Robust Generalization]
34. `ARXIV` [Towards understanding fast adversarial training]
35. `ARXIV` [Overfitting in adversarially robust deep learning]
36. `ICLR` [Robust local features for improving the generalization of adversarial training]
37. `ICML` [Confidence-Calibrated Adversarial Training: Generalizing to Unseen Attacks]
38. `ARXIV` [Regularizers for single-step adversarial training]
39. `CVPR` [Single-step adversarial training with dropout scheduling]
40. `ICLR` [Improving Adversarial Robustness Requires Revisiting Misclassified Examples]
41. `ARXIV` [Fast is better than free: Revisiting adversarial training.]
42. `ARXIV` [On the Generalization Properties of Adversarial Training]
43. `ARXIV` [A closer look at accuracy vs. robustness]
44. `ICLR` [Adversarially robust transfer learning]
45. `ARXIV` [On Saliency Maps and Adversarial Robustness]
46. `ARXIV` [On Detecting Adversarial Inputs with Entropy of Saliency Maps]
47. `ARXIV` [Detecting Adversarial Perturbations with Saliency]
48. `ARXIV` [Detection Defense Against Adversarial Attacks with Saliency Map]
49. `ARXIV` [Model-based Saliency for the Detection of Adversarial Examples]
50. `CVPR` [Auxiliary Training: Towards Accurate and Robust Models]
51. `CVPR` [Single-step Adversarial training with Dropout Scheduling]
52. `CVPR` [Achieving Robustness in the Wild via Adversarial Mixing With Disentangled Representations]
53. `ICML` [Test-Time Training with Self-Supervision for Generalization under Distribution Shifts](https://yueatsprograms.github.io/ttt/home.html)
54. `NeurIPS` [Improving robustness against common corruptions by covariate shift adaptation]
55. `CCS` [Gotta Catch'Em All: Using Honeypots to Catch Adversarial Attacks on Neural Networks]
56. `ECCV` [A simple way to make neural networks robust against diverse image corruptions]
57. `CVPRW` [Role of Spatial Context in Adversarial Robustness for Object Detection]
58. `WACV` [Local Gradients Smoothing: Defense against localized adversarial attacks]


### 2021
1. `ARXIV` [On the Limitations of Denoising Strategies as Adversarial Defenses](./2021/On_the_Limitations_of_Denoising_Strategies_as_Adversarial_Defenses.md)
2. `AAAI` [Understanding catastrophic overfitting in single-step adversarial training]
3. `ICLR` [Bag of tricks for adversarial training]
4. `ARXIV` [Bridging the Gap Between Adversarial Robustness and Optimization Bias]
5. `ICLR` [Perceptual Adversarial Robustness: Defense Against Unseen Threat Models]
6. `AAAI` [Adversarial Robustness through Disentangled Representations]
7. `ARXIV` [Understanding Robustness of Transformers for Image Classification]
8. `CVPR` [Adversarial Robustness under Long-Tailed Distribution]
9. `ARXIV` [Adversarial Attacks are Reversible with Natural Supervision]
10. `AAAI` [Attribute-Guided Adversarial Training for Robustness to Natural Perturbations]
11. `ICLR` [LEARNING PERTURBATION SETS FOR ROBUST MACHINE LEARNING]
12. `ICLR` [Improving Adversarial Robustness via Channel-wise Activation Suppressing]
13. `AAAI` [Efficient Certification of Spatial Robustness]
14. `ARXIV` [Domain Invariant Adversarial Learning]
15. `ARXIV` [Learning Defense Transformers for Counterattacking Adversarial Examples]
16. `ICLR` [ONLINE ADVERSARIAL PURIFICATION BASED ON SELF-SUPERVISED LEARNING]
17. `ARXIV` [Removing Adversarial Noise in Class Activation Feature Space]
18. `ARXIV` [Improving Adversarial Robustness Using Proxy Distributions]
19. `ARXIV` [Decoder-free Robustness Disentanglement without (Additional) Supervision]
20. `ARXIV` [Fighting Gradients with Gradients: Dynamic Defenses against Adversarial Attacks]
21. `ARXIV` [Reversible Adversarial Attack based on Reversible Image Transformation]
22. `ICLR` [ONLINE ADVERSARIAL PURIFICATION BASED ON SELF-SUPERVISED LEARNING]
23. `ARXIV` [Towards Corruption-Agnostic Robust Domain Adaptation]
24. `ARXIV` [Adversarially Trained Models with Test-Time Covariate Shift Adaptation]
25. `ICLR workshop` [COVARIATE SHIFT ADAPTATION FOR ADVERSARIALLY ROBUST CLASSIFIER]
26. `ARXIV` [Self-Supervised Adversarial Example Detection by Disentangled Representation]
27. `AAAI` [Adversarial Defence by Diversified Simultaneous Training of Deep Ensembles]
28. `ARXIV` [Understanding Catastrophic Overfitting in Adversarial Training]
29. `ACM Trans. Multimedia Comput. Commun. Appl` [Towards Corruption-Agnostic Robust Domain Adaptation]
30. `ICLR` [TENT: FULLY TEST-TIME ADAPTATION BY ENTROPY MINIMIZATION]
31. `ARXIV` [Attacking Adversarial Attacks as A Defense]
32. `ICML` [Adversarial purification with Score-based generative models]
33. `ARXIV` [Adversarial Visual Robustness by Causal Intervention]
34. `CVPR` [MaxUp: Lightweight Adversarial Training With Data Augmentation Improves Neural Network Training]
35. `MM` [AdvFilter: Predictive Perturbation-aware Filtering against Adversarial Attack via Multi-domain Learning]
36. `CVPR` [Robust and Accurate Object Detection via Adversarial Learning]
37. `ARXIV` [Markpainting: Adversarial Machine Learning meets Inpainting]
38. `ICLR` [EFFICIENT CERTIFIED DEFENSES AGAINST PATCH ATTACKS ON IMAGE CLASSIFIERS]
39. `ARXIV` [Learning Defense Transformers for Counterattacking Adversarial Examples]
40. `ARXIV` [Towards Robust Vision Transformer]
41. `ARXIV` [Reveal of Vision Transformers Robustness against Adversarial Attacks]
42. `ARXIV` [Intriguing Properties of Vision Transformers]
43. `ARXIV` [Vision transformers are robust learners]
44. `ARXIV` [On Improving Adversarial Transferability of Vision Transformers]
45. `ARXIV` [On the adversarial robustness of visual transformers]
46. `ARXIV` [On the robustness of vision transformers to adversarial examples]
47. `ARXIV` [Understanding Robustness of Transformers for Image Classification]
48. `ARXIV` [Regional Adversarial Training for Better Robust Generalization]
49. `CCS` [DetectorGuard: Provably Securing Object Detectors against Localized Patch Hiding Attacks]
50. `ARXIV` [MODELLING ADVERSARIAL NOISE FOR ADVERSARIAL DEFENSE]
51. `ICCV` [Adversarial Example Detection Using Latent Neighborhood Graph]
52. `ARXIV` [Identification of Attack-Specific Signatures in Adversarial Examples]
53. `Neurips` [How Should Pre-Trained Language Models Be Fine-Tuned Towards Adversarial Robustness?]

## 4th-Class
1. `ICCV 2017` [CVAE-GAN: Fine-Grained Image Generation Through Asymmetric Training](./2017/CVAE-GAN_Fine-Grained_Image_Generation_Through_Asymmetric_Training.md)
2. `ICML 2016` [Autoencoding beyond pixels using a learned similarity metric](./2016/Autoencoding_beyond_pixels_using_a_learned_similarity_metric.md) 
3. `ARXIV 2019` [Natural Adversarial Examples](./2019/Natural_Adversarial_Examples.md)
4. `ICML 2017` [Conditional Image Synthesis with Auxiliary Classifier {GAN}s](./2017/Conditional_Image_Synthesis_with_Auxiliary_Classifier_GANs.md)
5. `ICCV 2019` [SinGAN: Learning a Generative Model From a Single Natural Image](./2019/SinGAN_Learning_a_Generative_Model_From_a_Single_Natural_Image.md)
6. `ICLR 2020` [Robust And Interpretable Blind Image Denoising Via Bias-Free Convolutional Neural Networks](./2020/Robust_And_Interpretable_Blind_Image_Denoising_Via_Bias_Free_Convolutional_Neural_Networks.md)
7. `ICLR 2020` [Pay Attention to Features, Transfer Learn Faster CNNs](./2020/Pay_Attention_to_Features_Transfer_Learn_Faster_CNNs.md)
8. `ICLR 2020` [On Robustness of Neural Ordinary Differential Equations](./2020/On_Robustness_of_Neural_Ordinary_Differential_Equations.md)
9. `ICCV 2019` [Real Image Denoising With Feature Attention](./2019/Real_Image_Denoising_With_Feature_Attention.md)
10. `ICLR 2018` [Multi-Scale Dense Networks for Resource Efficient Image Classification](./2018/Multi_Scale_Dense_Networks_for_Resource_Efficient_Image_Classification.md)
11. `ARXIV 2019` [Rethinking Data Augmentation: Self-Supervision and Self-Distillation](2019/Rethinking_Data_Augmentation_Self_Supervision_and_Self_Distillation.md)
12. `ICCV 2019` [Be Your Own Teacher: Improve the Performance of Convolutional Neural Networks via Self Distillation](./2019/Be_Your_Own_Teacher_Improve%20the_Performance_of_Convolutional_Neural_Networks_via_Self_Distillation.md)
13. `ARXIV 2019` [Adversarially Robust Distillation](./2019/Adversarially_Robust_Distillation.md)
14. `ARXIV 2019` [Knowledge Distillation from Internal Representations](./2019/Knowledge_Distillation_from_Internal_Representations.md)
15. `ICLR 2020` [Contrastive Representation Distillation](./2020/Contrastive_Representation_Distillation.md) :thought_balloon:
16. `NIPS 2018` [Faster Neural Networks Straight from JPEG](./2018/Faster_Neural_Networks_Straight_from_JPEG.md)
17. `ARXIV 2019` [A Closer Look at Double Backpropagation](./2019/A_Closer_Look_at_Double_Backpropagation.md):thought_balloon:
18. `CVPR 2016` [Learning Deep Features for Discriminative Localization](./2016/Learning_Deep_Features_for_Discriminative_Localization.md)
19. `ICML 2019` [Noise2Self: Blind Denoising by Self-Supervision](./2019/Noise2Self_Blind_Denoising_by_Self_Supervision.md)
20. `ARXIV 2020` [Supervised Contrastive Learning](./2020/Supervised_Contrastive_Learning.md)
21. `CVPR 2020` [High-Frequency Component Helps Explain the Generalization of Convolutional Neural Networks](./2020/High_Frequency_Component_Helps_Explain_the_Generalization_of_Convolutional_Neural_Networks.md)
22. `NIPS 2017` [Counterfactual Fairness]
23. `ARXIV 2020` [An Adversarial Approach for Explaining the Predictions of Deep Neural Networks]
24. `CVPR 2014` [Rich feature hierarchies for accurate object detection and semantic segmentation]
25. `ICLR 2018` [Spectral Normalization for Generative Adversarial Networks]
26. `NIPS 2018` [MetaGAN: An Adversarial Approach to Few-Shot Learning]
27. `ARXIV 2019` [Breaking the cycle -- Colleagues are all you need]
28. `ARXIV 2019` [LOGAN: Latent Optimisation for Generative Adversarial Networks]
29. `ICML 2020` [Margin-aware Adversarial Domain Adaptation with Optimal Transport] 
30. `ICML 2020` [Representation Learning Using Adversarially-Contrastive Optimal Transport]
31. `ICLR 2021` [Free Lunch for Few-shot Learning: Distribution Calibration]
32. `CVPR 2019` [Unprocessing Images for Learned Raw Denoising]
33. `TPAMI 2020` [Image Quality Assessment: Unifying Structure and Texture Similarity]
34. `CVPR 2020` [Dreaming to Distill: Data-free Knowledge Transfer via DeepInversion]
35. `ICLR 2021` [WHAT SHOULD NOT BE CONTRASTIVE IN CONTRASTIVE LEARNING]
36. `ARXIV` [MT3: Meta Test-Time Training for Self-Supervised Test-Time Adaption]
37. `ARXIV` [UNSUPERVISED DOMAIN ADAPTATION THROUGH SELF-SUPERVISION]
38. `ARXIV` [Estimating Example Difficulty using Variance of Gradients]


## Links
- [Adversarial Machine Learning Reading List](https://nicholas.carlini.com/writing/2018/adversarial-machine-learning-reading-list.html) by [Nicholas Carlini](https://nicholas.carlini.com)
- [A Complete List of All (arXiv) Adversarial Example Papers](https://nicholas.carlini.com/writing/2019/all-adversarial-example-papers.html) by [Nicholas Carlini](https://nicholas.carlini.com) **Stay Tuned** 
