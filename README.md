![Maintenance](https://img.shields.io/maintenance/yes/2021.svg?color=red&style=plastic)
![GitHub last commit](https://img.shields.io/github/last-commit/tao-bai/attack-and-defense-methods.svg?style=plastic)
[![Awesome](https://awesome.re/badge.svg?style=flat-square)](https://awesome.re)

# bib_adversarial_examples
Literature collection of adversarial examples.

As continuation from this [collection](https://github.com/tao-bai/attack-and-defense-methods).

# Papers

## Surveys
2022
 - `IEEE` [ANOMALOUS INSTANCE DETECTION IN DEEP LEARNING: A SURVEY](https://www.osti.gov/biblio/1631092)


2021
 - `Journal` [https://paperswithcode.com/paper/adversarial-attacks-and-defenses-on-graphs-a](https://link.springer.com/content/pdf/10.1007/s11633-019-1211-x.pdf)
 - [Countering Malicious DeepFakes: Survey, Battleground, and Horizon](https://arxiv.org/pdf/2103.00218.pdf)


2020
 - `IEEE` [Anomalous Example Detection in Deep Learning: A Survey](https://www.osti.gov/biblio/1631092)

2018
 - [Adversarial Examples: Attacks and Defenses for Deep Learning](https://arxiv.org/pdf/1712.07107.pdf), [github](https://github.com/chbrian/awesome-adversarial-examples-dl)
 - [Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples](https://github.com/anishathalye/obfuscated-gradients)

2017
 - [Towards Evaluating the Robustness of Neural Networks](https://arxiv.org/pdf/1608.04644.pdf)

## Attacks

2021
 - `Preprint` [Membership Inference Attacks From First Principles](https://arxiv.org/abs/2112.03570#:~:text=A%20membership%20inference%20attack%20allows,in%20the%20model's%20training%20dataset.)

2017
 - `Reject` [Exploring the Space of Black-box Attacks on Deep Neural Networks ](https://openreview.net/forum?id=SkF2D7g0b)

2016
 - `CVPR` [DeepFool: a simple and accurate method to fool deep neural networks](https://arxiv.org/abs/1511.04599)

## Defenses

2021
 - `ICML` + code [A General Framework For Detecting Anomalous Inputs to DNN Classifiers](https://github.com/jayaram-r/adversarial-detection) + [talk](https://slideslive.com/38958657/a-general-framework-for-detecting-anomalous-inputs-to-dnn-classifiers?ref=account-folder-86375-folders) + [slides](https://icml.cc/media/icml-2021/Slides/10151.pdf)
 - `ICME` [DefakeHop: A Light-Weight High-Performance Deepfake Detector](https://arxiv.org/abs/2103.06929)
 - [Detecting Adversarial Examples with Bayesian Neural Network](https://arxiv.org/abs/2105.08620)
 - `CVPR` + [code](https://github.com/thudzj/ScalableBDL) [LiBRe: A Practical Bayesian Approach to Adversarial Detection](https://openaccess.thecvf.com/content/CVPR2021/papers/Deng_LiBRe_A_Practical_Bayesian_Approach_to_Adversarial_Detection_CVPR_2021_paper.pdf)
 - `Thesis` [Defense Methods for Convolutional Neural Networks Against Adversarial
Attacks](https://repositorio.ufsc.br/bitstream/handle/123456789/226929/PEAS0369-D.pdf?sequence=-1)

2020
 - [Adversarial Detection and Correction by Matching Prediction Distributions](https://arxiv.org/pdf/2002.09364.pdf)

2019
 - `ICML` [The odds ard odd](https://qdata.github.io/deep2Read/deep2reproduce/2019Fall/T23_Stein_Merielms7nk_Detecting_Adversarial_Examples.pdf)
 - `ICLR` [AD V-BNN: IMPROVED ADVERSARIAL DEFENSE THROUGH ROBUST BAYESIAN NEURAL NETWORK](https://openreview.net/pdf?id=rk4Qso0cKm)

2018
 - `NeurIPS` [Code + Trust Score](https://github.com/google/TrustScore)

2017
 - `ICLR` [ON DETECTING ADVERSARIAL PERTURBATIONS](https://arxiv.org/pdf/1702.04267.pdf)





2019
 - `CVPR` [Detection based Defense against Adversarial Examples from the Steganalysis Point of View](https://openaccess.thecvf.com/content_CVPR_2019/papers/Liu_Detection_Based_Defense_Against_Adversarial_Examples_From_the_Steganalysis_Point_CVPR_2019_paper.pdf)
 - `NeurIPS`  [Adversarial Examples Are Not Bugs, They Are Features](https://arxiv.org/pdf/1905.02175.pdf)


## Adversarial Training

2021
 - `NeurIPS` [Do Wider Neural Networks Really Help Adversarial Robustness?](https://openreview.net/forum?id=wxjtOI_8jO)
 - `ICML`    [Meta Adversarial Training against Universal Patches](https://openreview.net/forum?id=sePThSlRHr)
 - `ICML` [Adversarially Trained Neural Policies in the Fourier Domain](https://openreview.net/pdf?id=bXrbdIoYEj)

## Detection in Image Segmentation

2022

 - `Preprint` [DAAIN: Detection of Anomalous and Adversarial Input using Normalizing Flows](https://github.com/merantix/mxlabs-daain)
 - `Preprint` [Adversarial Examples on Segmentation Models Can be Easy to Transfer](https://arxiv.org/pdf/2111.11368.pdf)

2021
 - `IJCNN` [Studying the Transferability of Non-Targeted Adversarial Attacks](https://ieeexplore.ieee.org/document/9534138)

2018
 - `CVPR` [On the Robustness of Semantic Segmentation Models to Adversarial Attacks](https://ora.ox.ac.uk/objects/uuid:d261589f-2717-41ee-9fa4-17bc831229cc)

## Datasets

 - https://github.com/MadryLab/constructed-datasets 


## Others

2022
 - `Preprint` [Evaluation of Neural Networks defenses and attacks using NDCG and reciprocal rank metrics](https://arxiv.org/pdf/2201.05071.pdf)
 - `Preprint` [Performance Evaluation of Adversarial Attacks: Discrepancies and Solutions](https://arxiv.org/pdf/2104.11103.pdf)


2021
- `CVPR` [Natural Adversarial Examples](https://arxiv.org/abs/1907.07174)
- https://simons.berkeley.edu/sites/default/files/docs/11887/nn-simons-part2.pdf
- [FFT slides](https://courses.engr.illinois.edu/cs445/fa2020/lectures/Lecture%2003%20-%20Thinking%20in%20Frequency%20-%20Online.pdf)
- `Preprint` [Closer Look at the Transferability of Adversarial Examples: How They Fool Different Models Differently](https://www.researchgate.net/publication/357417434_Closer_Look_at_the_Transferability_of_Adversarial_Examples_How_They_Fool_Different_Models_Differently)

2019
 - `NeurIPS` [A Fourier Perspective on Model Robustness in Computer Vision](https://proceedings.neurips.cc/paper/2019/file/b05b57f6add810d3b7490866d74c0053-Paper.pdf)

 - [Understanding Benfordâ€™s law and its vulnerability in image forensics](http://www.cs.albany.edu/~patrey/ICSI533-433/project/Implementation_sample_report.pdf)

## Libs
 - [DeepRobust](https://github.com/DSE-MSU/DeepRobust)
 - [Innsight](https://bips-hb.github.io/innsight/)
 - [RobustBench](https://robustbench.github.io/)
 - [AdverTorch](https://github.com/BorealisAI/advertorch)
 - [MadryLab](https://github.com/MadryLab)
 - [Foolbox](https://github.com/bethgelab/foolbox)


[adversarial-attacks-and-defense](https://github.com/cuge1995/NeurIPS-2021-adversarial-attacks-and-defense-)

## Links

- [Adversarial Machine Learning Reading List](https://nicholas.carlini.com/writing/2018/adversarial-machine-learning-reading-list.html) by [Nicholas Carlini](https://nicholas.carlini.com)
- [A Complete List of All (arXiv) Adversarial Example Papers](https://nicholas.carlini.com/writing/2019/all-adversarial-example-papers.html) by [Nicholas Carlini](https://nicholas.carlini.com) **Stay Tuned** 

